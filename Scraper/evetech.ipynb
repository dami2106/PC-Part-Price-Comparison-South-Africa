{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time \n",
    "from selenium.webdriver.support.ui import Select\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import requests\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Chrome options for headless mode and different user agent\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # Enable headless mode\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "\n",
    "# Create a webdriver instance with the specified options\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Navigate to the desired website\n",
    "url = 'https://www.evetech.co.za/components.aspx'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "def scrape_site(driver,categories):\n",
    "    time.sleep(5)\n",
    "    # Get the page source (HTML content)\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Use BeautifulSoup to parse the HTML\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    #Product category\n",
    "    product_category = soup.find('div',class_='d-block cols-12 gap-2 gap-sm-3 comp-top-section')\n",
    "    product_category = product_category.findAll('div', class_='detail')\n",
    "    get_len_of_product_category = len(product_category)\n",
    "    if len(product_category) < 1:\n",
    "        product_category = categories\n",
    "    else:\n",
    "        product_category = product_category[get_len_of_product_category-1].find('h1').text\n",
    "\n",
    "    # Find the number of product\n",
    "    products = soup.findAll('div', class_='ComponentCard_Products__Card__SJT5q ComponentCard_HoverGrow__Q2lEZ shadow overflow-hidden h-100 gap-2 position-relative card')\n",
    "    print(\"Number of products:\", len(products))\n",
    "\n",
    "    for i in products:\n",
    "        # Get the product name and price and availability \n",
    "        product_names = i.find('h3', class_=\"fs-6 fw-2 lh-1 m-0 overflow-hidden h-100\").text.strip()\n",
    "        product_price = int(i.find('div', class_=\"ComponentCard_Products__Price__SG2mn fw-3 fs-3 flex-shrink-0\").text.replace(\"R \", \"\").strip())\n",
    "\n",
    "        product_availiable_text = str(i.find('span', class_=\"fw-1 fs-6 text-wrap\").text.strip())\n",
    "        # print(product_availiable_text)\n",
    "        product_availiablity = True if product_availiable_text.__contains__(\"In Stock\") else False\n",
    "\n",
    "        all_data.append([product_names, product_price, product_availiablity, product_category])\n",
    "        # print(product_names, \": \", product_price, \": \", product_availiablity,\": \", product_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the page source (HTML content)\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Use BeautifulSoup to parse the HTML\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find all categories button \n",
    "categories = soup.findAll('div', class_='Components_Child__mYntX Components_HoverGrow__br6Zs position-relative')\n",
    "page_positioin = 250\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    # Find all buttons within the category div\n",
    "    buttons = category.find_all('button', class_='rounded-pill bg-gradient lh-1 border border-primary btn btn-light btn-sm')\n",
    "\n",
    "    # Check if there is a button with view all\n",
    "    button_view_all = category.find('button', class_='rounded-pill bg-gradient lh-1 btn btn-primary btn-sm')\n",
    "\n",
    "    # Check if the button is not None\n",
    "    if button_view_all is not None:\n",
    "        get_number_of_buttons = len(buttons)\n",
    "        # You can construct XPath using the button class if needed\n",
    "        button_xpath = f'//*[@id=\"root\"]/div/div[2]/div/section[2]/div/div/div[{i+1}]/div[1]/div[1]/button[{get_number_of_buttons+1}]'\n",
    "\n",
    "        # Scroll to the button\n",
    "        driver.execute_script(f\"window.scrollTo(0, {str(page_positioin)});\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Click the button\n",
    "        driver.find_element(By.XPATH, button_xpath).click()\n",
    "        print(\"Scraping:\", driver.current_url)\n",
    "        # Scrape the site\n",
    "        scrape_site(driver, button_view_all.text)\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Go back to the previous page and scroll to the button\n",
    "        driver.execute_script(\"window.history.go(-1)\")\n",
    "        time.sleep(5)\n",
    "        driver.execute_script(f\"window.scrollTo(0, {str(page_positioin)});\")\n",
    "        time.sleep(5)\n",
    "    else:\n",
    "        for k, button in enumerate(buttons):\n",
    "            # You can construct XPath using the button class if needed\n",
    "            button_xpath = f'//*[@id=\"root\"]/div/div[2]/div/section[2]/div/div/div[{i+1}]/div[1]/div[1]/button[{k+1}]'\n",
    "\n",
    "            # Scroll to the button\n",
    "            driver.execute_script(f\"window.scrollTo(0, {str(page_positioin)});\")\n",
    "            time.sleep(7)\n",
    "\n",
    "            # Click the button\n",
    "            driver.find_element(By.XPATH, button_xpath).click()\n",
    "            print(\"Scraping:\", driver.current_url)\n",
    "            # Scrape the site\n",
    "            scrape_site(driver, button.text)\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Go back to the previous page and scroll to the button\n",
    "            driver.execute_script(\"window.history.go(-1)\")\n",
    "            time.sleep(5)\n",
    "            driver.execute_script(f\"window.scrollTo(0, {str(page_positioin)});\")\n",
    "            time.sleep(5)\n",
    "    \n",
    "    # Get the page position\n",
    "    if (i+1) % 3 == 0:\n",
    "        page_positioin += 250\n",
    "        \n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping taking around 32 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Write the data to csv file\n",
    "with open('../Products/evetech.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Title','Price','In Stock','Category'])\n",
    "    writer.writerows(all_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scrapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
