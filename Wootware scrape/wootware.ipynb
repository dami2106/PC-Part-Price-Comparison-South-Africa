{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time \n",
    "from selenium.webdriver.support.ui import Select\n",
    "from datetime import datetime\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 products on the page Graphics Cards.\n",
      "Found 50 products on the page Graphics Cards.\n",
      "Found 50 products on the page Graphics Cards.\n",
      "Found 50 products on the page Graphics Cards.\n",
      "Found 50 products on the page Graphics Cards.\n",
      "Found 50 products on the page Graphics Cards.\n",
      "Found 50 products on the page Graphics Cards.\n",
      "Found 50 products on the page Graphics Cards.\n",
      "Found 21 products on the page Graphics Cards.\n",
      "Found 50 products on the page Processors / CPUs.\n",
      "Found 50 products on the page Processors / CPUs.\n",
      "Found 14 products on the page Processors / CPUs.\n",
      "Found 50 products on the page Memory / RAM.\n",
      "Found 50 products on the page Memory / RAM.\n",
      "Found 50 products on the page Memory / RAM.\n",
      "Found 50 products on the page Memory / RAM.\n",
      "Found 50 products on the page Memory / RAM.\n",
      "Found 50 products on the page Memory / RAM.\n",
      "Found 50 products on the page Memory / RAM.\n",
      "Found 50 products on the page Memory / RAM.\n",
      "Found 50 products on the page Memory / RAM.\n",
      "Found 50 products on the page Memory / RAM.\n",
      "Found 50 products on the page Memory / RAM.\n",
      "Found 15 products on the page Memory / RAM.\n",
      "Found 50 products on the page Motherboards.\n",
      "Found 50 products on the page Motherboards.\n",
      "Found 50 products on the page Motherboards.\n",
      "Found 50 products on the page Motherboards.\n",
      "Found 50 products on the page Motherboards.\n",
      "Found 50 products on the page Motherboards.\n",
      "Found 50 products on the page Motherboards.\n",
      "Found 50 products on the page Motherboards.\n",
      "Found 6 products on the page Motherboards.\n",
      "Found 50 products on the page Power Supplies / PSUs.\n",
      "Found 50 products on the page Power Supplies / PSUs.\n",
      "Found 50 products on the page Power Supplies / PSUs.\n",
      "Found 50 products on the page Power Supplies / PSUs.\n",
      "Found 7 products on the page Power Supplies / PSUs.\n",
      "Found 50 products on the page Cases / Chassis.\n",
      "Found 50 products on the page Cases / Chassis.\n",
      "Found 50 products on the page Cases / Chassis.\n",
      "Found 50 products on the page Cases / Chassis.\n",
      "Found 50 products on the page Cases / Chassis.\n",
      "Found 50 products on the page Cases / Chassis.\n",
      "Found 50 products on the page Cases / Chassis.\n",
      "Found 16 products on the page Cases / Chassis.\n",
      "Found 50 products on the page Solid State Drives / SSDs.\n",
      "Found 50 products on the page Solid State Drives / SSDs.\n",
      "Found 50 products on the page Solid State Drives / SSDs.\n",
      "Found 50 products on the page Solid State Drives / SSDs.\n",
      "Found 50 products on the page Solid State Drives / SSDs.\n",
      "Found 50 products on the page Solid State Drives / SSDs.\n",
      "Found 50 products on the page Solid State Drives / SSDs.\n",
      "Found 50 products on the page Solid State Drives / SSDs.\n",
      "Found 50 products on the page Internal Hard Drives.\n",
      "Found 50 products on the page Internal Hard Drives.\n",
      "Found 13 products on the page Internal Hard Drives.\n",
      "Found 50 products on the page External Hard Drives.\n",
      "Found 25 products on the page External Hard Drives.\n",
      "Found 7 products on the page CD/DVD/Blu-Ray Burners & Media.\n",
      "Found 50 products on the page Flash Memory / USB & Card Readers.\n",
      "Found 43 products on the page Flash Memory / USB & Card Readers.\n",
      "Found 31 products on the page Hard Drive Enclosures.\n",
      "Found 4 products on the page Hard Drive Docks.\n",
      "Found 50 products on the page Monitors / Screens.\n",
      "Found 50 products on the page Monitors / Screens.\n",
      "Found 50 products on the page Monitors / Screens.\n",
      "Found 50 products on the page Monitors / Screens.\n",
      "Found 50 products on the page Monitors / Screens.\n",
      "Found 50 products on the page Monitors / Screens.\n",
      "Found 3 products on the page Monitors / Screens.\n",
      "Found 50 products on the page Keyboards, Mice & Controllers.\n",
      "Found 50 products on the page Keyboards, Mice & Controllers.\n",
      "Found 50 products on the page Keyboards, Mice & Controllers.\n",
      "Found 50 products on the page Keyboards, Mice & Controllers.\n",
      "Found 50 products on the page Keyboards, Mice & Controllers.\n",
      "Found 50 products on the page Keyboards, Mice & Controllers.\n",
      "Found 50 products on the page Keyboards, Mice & Controllers.\n",
      "Found 50 products on the page Keyboards, Mice & Controllers.\n",
      "Found 50 products on the page Keyboards, Mice & Controllers.\n",
      "Found 50 products on the page Keyboards, Mice & Controllers.\n",
      "Found 50 products on the page Keyboards, Mice & Controllers.\n",
      "Found 50 products on the page Keyboards, Mice & Controllers.\n",
      "Found 7 products on the page Keyboards, Mice & Controllers.\n",
      "Found 50 products on the page Headsets, Speakers & Mics.\n",
      "Found 50 products on the page Headsets, Speakers & Mics.\n",
      "Found 50 products on the page Headsets, Speakers & Mics.\n",
      "Found 50 products on the page Headsets, Speakers & Mics.\n",
      "Found 50 products on the page Headsets, Speakers & Mics.\n",
      "Found 20 products on the page Headsets, Speakers & Mics.\n",
      "Found 9 products on the page Projectors.\n",
      "Found 7 products on the page Soundcards.\n",
      "Found 50 products on the page Printers, Scanners & Fax.\n",
      "Found 50 products on the page Printers, Scanners & Fax.\n",
      "Found 50 products on the page Printers, Scanners & Fax.\n",
      "Found 5 products on the page Printers, Scanners & Fax.\n",
      "Found 50 products on the page Ink & Toner.\n",
      "Found 14 products on the page Ink & Toner.\n",
      "Found 3 products on the page Modems.\n",
      "Found 50 products on the page Routers.\n",
      "Found 44 products on the page Routers.\n",
      "Found 47 products on the page Adapters.\n",
      "Found 40 products on the page Switches.\n",
      "Found 44 products on the page Wireless Networks.\n",
      "Found 19 products on the page Network Attached Storage.\n",
      "Found 50 products on the page Water / Liquid Cooling.\n",
      "Found 50 products on the page Water / Liquid Cooling.\n",
      "Found 50 products on the page Water / Liquid Cooling.\n",
      "Found 9 products on the page Water / Liquid Cooling.\n",
      "Found 50 products on the page Fans & CPU Coolers.\n",
      "Found 50 products on the page Fans & CPU Coolers.\n",
      "Found 50 products on the page Fans & CPU Coolers.\n",
      "Found 50 products on the page Fans & CPU Coolers.\n",
      "Found 50 products on the page Fans & CPU Coolers.\n",
      "Found 50 products on the page Fans & CPU Coolers.\n",
      "Found 50 products on the page Fans & CPU Coolers.\n",
      "Found 50 products on the page Fans & CPU Coolers.\n",
      "Found 50 products on the page Fans & CPU Coolers.\n",
      "Found 20 products on the page Fans & CPU Coolers.\n",
      "Found 50 products on the page UPS / Power Protection.\n",
      "Found 40 products on the page UPS / Power Protection.\n",
      "Found 50 products on the page Cables.\n",
      "Found 50 products on the page Cables.\n",
      "Found 50 products on the page Cables.\n",
      "Found 28 products on the page Cables.\n",
      "Found 50 products on the page Other.\n",
      "Found 50 products on the page Other.\n",
      "Found 50 products on the page Other.\n",
      "Found 50 products on the page Other.\n",
      "Found 50 products on the page Other.\n",
      "Found 50 products on the page Other.\n",
      "Found 50 products on the page Other.\n",
      "Found 38 products on the page Other.\n",
      "Found 13 products on the page Tools.\n",
      "Found 38 products on the page Software.\n",
      "Found 27 products on the page Webcams.\n",
      "Found 20 products on the page Cellphones.\n",
      "Found 14 products on the page Consoles.\n",
      "Found 29 products on the page Gaming Chairs.\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://www.wootware.co.za/'\n",
    "\n",
    "# Parse the HTML\n",
    "home_response = requests.get(URL)\n",
    "home_soup = BeautifulSoup(home_response.text, 'html.parser')\n",
    "\n",
    "# Create csv file\n",
    "csv_file = open('wootware.csv', 'w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['Product Name','Product Availability','Product Price', 'Product Category'])\n",
    "\n",
    "# Find all the categories\n",
    "nav_toggler = home_soup.find('div', class_='nav-dropdown level0')\n",
    "different_categories = nav_toggler.findAll('a', class_='ww-block ww-text-base ww-text-gray-500 hover:ww-text-amber-500 ww-py-0.5 ww-no-underline ww-whitespace-nowrap')\n",
    "\n",
    "# Iterate through the categories\n",
    "for i in different_categories:\n",
    "   \n",
    "    #Every category name and link\n",
    "    category_name = i.text.strip()\n",
    "    URL = i['href']\n",
    "\n",
    "    #Open the category link\n",
    "    stopping = False\n",
    "    while stopping == False:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(URL)\n",
    "        \n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all the products on the page\n",
    "            products = soup.find_all('div', class_=\"main-info\")\n",
    "\n",
    "            #@TODO: Remove this print when finalize\n",
    "            print(f\"Found {len(products)} products on the page {category_name}.\")\n",
    "\n",
    "            # Iterate through the products\n",
    "            for product in products:\n",
    "                # Get the product name\n",
    "                product_name = product.find('h2', class_=\"product-name\").text.strip()\n",
    "\n",
    "                # Get the product availability\n",
    "                if product.find('div', class_=\"availability-in-stock\") is None:\n",
    "                    product_availability = \"Out of Stock\"\n",
    "                else:\n",
    "                    product_availability = product.find('div', class_=\"availability-in-stock\").text.strip()\n",
    "\n",
    "                # Get the product price\n",
    "                product_price = -99\n",
    "                if product_availability != \"Out of Stock\":\n",
    "                    price_box = product.find('div', class_=\"price-box\")\n",
    "\n",
    "                    # Get all the prices\n",
    "                    all_prices = price_box.find_all('span', class_=\"price\")\n",
    "                    if len(all_prices) == 1:\n",
    "                        product_price = all_prices[0].text.strip()\n",
    "                    else:\n",
    "                        product_price = all_prices[1].text.strip()\n",
    "                    product_price = product_price.replace(\"R\", \"\").replace(\",\", \"\").strip()\n",
    "                \n",
    "                #save to csv\n",
    "                csv_writer.writerow([product_name, product_availability, product_price, category_name])\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "        # Check if there is a next page\n",
    "        if soup.find('a', class_=\"next i-next\") is not None:\n",
    "            nextpage = soup.find('a', class_=\"next i-next\")\n",
    "            URL = nextpage['href']\n",
    "        else:\n",
    "            stopping = True\n",
    "        time.sleep(2)\n",
    "    time.sleep(10)\n",
    "\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL = 'https://www.wootware.co.za/'\n",
    "\n",
    "# # Parse the HTML\n",
    "# home_response = requests.get(URL)\n",
    "# home_soup = BeautifulSoup(home_response.text, 'html.parser')\n",
    "\n",
    "# # Create csv file\n",
    "# csv_file = open('wootware.csv', 'w', newline='', encoding='utf-8')\n",
    "# csv_writer = csv.writer(csv_file)\n",
    "# csv_writer.writerow(['Product Name','Product Availability','Product Price', 'Product Category'])\n",
    "\n",
    "# # Find all the categories\n",
    "# nav_toggler = home_soup.find('div', class_='nav-dropdown level0')\n",
    "# different_categories = nav_toggler.findAll('a', class_='ww-block ww-text-base ww-text-gray-500 hover:ww-text-amber-500 ww-py-0.5 ww-no-underline ww-whitespace-nowrap')\n",
    "\n",
    "# # Iterate through the categories\n",
    "# for i in different_categories:\n",
    "   \n",
    "#     #Every category name and link\n",
    "#     category_name = i.text.strip()\n",
    "#     URL = i['href']\n",
    "\n",
    "#     #Open the category link\n",
    "#     stopping = False\n",
    "#     while stopping == False:\n",
    "#         # Send a GET request to the URL\n",
    "#         response = requests.get(URL)\n",
    "        \n",
    "#         # Check if the request was successful (status code 200)\n",
    "#         if response.status_code == 200:\n",
    "#             # Parse the HTML content of the page\n",
    "#             soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#             # Find all the products on the page\n",
    "#             products = soup.find_all('div', class_=\"main-info\")\n",
    "\n",
    "#             #@TODO: Remove this print when finalize\n",
    "#             print(f\"Found {len(products)} products on the page {category_name}.\")\n",
    "\n",
    "#             # Iterate through the products\n",
    "#             for product in products:\n",
    "#                 # Get the product name\n",
    "#                 product_name = product.find('h2', class_=\"product-name\").text.strip()\n",
    "\n",
    "#                 # Get the product availability\n",
    "#                 if product.find('div', class_=\"availability-in-stock\") is None:\n",
    "#                     product_availability = \"Out of Stock\"\n",
    "#                 else:\n",
    "#                     product_availability = product.find('div', class_=\"availability-in-stock\").text.strip()\n",
    "\n",
    "#                 # Get the product price\n",
    "#                 product_price = -99\n",
    "#                 if product_availability != \"Out of Stock\":\n",
    "#                     price_box = product.find('div', class_=\"price-box\")\n",
    "\n",
    "#                     # Get all the prices\n",
    "#                     all_prices = price_box.find_all('span', class_=\"price\")\n",
    "#                     if len(all_prices) == 1:\n",
    "#                         product_price = all_prices[0].text.strip()\n",
    "#                     else:\n",
    "#                         product_price = all_prices[1].text.strip()\n",
    "#                     product_price = product_price.replace(\"R\", \"\").replace(\",\", \"\").strip()\n",
    "                \n",
    "#                 #save to csv\n",
    "#                 csv_writer.writerow([product_name, product_availability, product_price, category_name])\n",
    "\n",
    "#         else:\n",
    "#             print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "#         # Check if there is a next page\n",
    "#         if soup.find('a', class_=\"next i-next\") is not None:\n",
    "#             nextpage = soup.find('a', class_=\"next i-next\")\n",
    "#             URL = nextpage['href']\n",
    "#         else:\n",
    "#             stopping = True\n",
    "#         time.sleep(2)\n",
    "#     time.sleep(10)\n",
    "\n",
    "# csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 products on the page.\n",
      "Found 50 products on the page.\n",
      "Found 50 products on the page.\n",
      "Found 50 products on the page.\n",
      "Found 50 products on the page.\n",
      "Found 50 products on the page.\n",
      "Found 50 products on the page.\n",
      "Found 50 products on the page.\n",
      "Found 21 products on the page.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#create csv file\n",
    "csv_file = open('wootware.csv', 'w', newline='', encoding='utf-8')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['Product Name','Product Availability','Product Price'])\n",
    "\n",
    "URL = 'https://www.wootware.co.za/computer-hardware/video-cards-video-devices'\n",
    "stopping = False\n",
    "while stopping == False:\n",
    "    \n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(URL)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of tahe page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        products = soup.find_all('div', class_=\"main-info\")\n",
    "        print(f\"Found {len(products)} products on the page.\")\n",
    "        for product in products:\n",
    "            # print(product)\n",
    "            product_name = product.find('h2', class_=\"product-name\").text.strip()\n",
    "            if product.find('div', class_=\"availability-in-stock\") is None:\n",
    "                product_availability = \"Out of Stock\"\n",
    "            else:\n",
    "                product_availability = product.find('div', class_=\"availability-in-stock\").text.strip()\n",
    "\n",
    "            product_price = -99\n",
    "            if product_availability != \"Out of Stock\":\n",
    "                price_box = product.find('div', class_=\"price-box\")\n",
    "                all_prices = price_box.find_all('span', class_=\"price\")\n",
    "                if len(all_prices) == 1:\n",
    "                    product_price = all_prices[0].text.strip()\n",
    "                else:\n",
    "                    product_price = all_prices[1].text.strip()\n",
    "                product_price = product_price.replace(\"R\", \"\").replace(\",\", \"\").strip()\n",
    "            \n",
    "            #save to csv\n",
    "            csv_writer.writerow([product_name, product_availability, product_price])\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "    if soup.find('a', class_=\"next i-next\") is not None:\n",
    "        nextpage = soup.find('a', class_=\"next i-next\")\n",
    "        URL = nextpage['href']\n",
    "    else:\n",
    "        stopping = True\n",
    "\n",
    "csv_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scrapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
